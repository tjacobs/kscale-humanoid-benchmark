{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# K-Scale Humanoid Benchmark\n",
    "\n",
    "Welcome to the K-Scale Humanoid Benchmark! This notebook will walk you through training your own reinforcement learning policy, which you can then use to control a K-Scale robot.\n",
    "\n",
    "*Note:* The Just-In-Time compilation may take a while and cause your Colab instance to appear to disconnect. However, your training cell may actually still be running. Make sure to check before restarting!\n",
    "\n",
    "*Last updated: 2025/05/09*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "GcUyV2BhHCxS"
   },
   "source": [
    "## Dependencies and Config\n",
    "\n",
    "The K-Scale Humanoid Benchmark uses K-Scale's open-source RL framework [K-Sim](https://github.com/kscalelabs/ksim) for training and the [K-Scale API](https://github.com/kscalelabs/kscale) for asset management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9GR-PWjgynB",
    "outputId": "8b5227f7-e06e-465e-ce65-ed19f75d1191"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "!pip install ksim kos-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19e07786",
    "outputId": "78e173ed-afd3-45fc-a518-2a4a2ba31b8f"
   },
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "%env TENSORBOARD_PORT=6036\n",
    "%env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Self\n",
    "\n",
    "import attrs\n",
    "import distrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ksim\n",
    "import mujoco\n",
    "import mujoco_scenes\n",
    "import mujoco_scenes.mjcf\n",
    "import optax\n",
    "import xax\n",
    "from jaxtyping import Array, PRNGKeyArray\n",
    "from kscale.web.gen.api import JointMetadataOutput, RobotURDFMetadataOutput\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "NUM_JOINTS = 20\n",
    "NUM_ACTOR_INPUTS = 49\n",
    "NUM_CRITIC_INPUTS = 444\n",
    "\n",
    "# These are in the order of the neural network outputs.\n",
    "ZEROS: list[tuple[str, float]] = [\n",
    "    (\"dof_right_shoulder_pitch_03\", 0.0),\n",
    "    (\"dof_right_shoulder_roll_03\", math.radians(-10.0)),\n",
    "    (\"dof_right_shoulder_yaw_02\", 0.0),\n",
    "    (\"dof_right_elbow_02\", math.radians(15.0)),\n",
    "    (\"dof_right_wrist_00\", 0.0),\n",
    "    (\"dof_left_shoulder_pitch_03\", 0.0),\n",
    "    (\"dof_left_shoulder_roll_03\", math.radians(10.0)),\n",
    "    (\"dof_left_shoulder_yaw_02\", 0.0),\n",
    "    (\"dof_left_elbow_02\", math.radians(-15.0)),\n",
    "    (\"dof_left_wrist_00\", 0.0),\n",
    "    (\"dof_right_hip_pitch_04\", math.radians(-25.0)),\n",
    "    (\"dof_right_hip_roll_03\", math.radians(-5.0)),\n",
    "    (\"dof_right_hip_yaw_03\", 0.0),\n",
    "    (\"dof_right_knee_04\", math.radians(-50.0)),\n",
    "    (\"dof_right_ankle_02\", math.radians(25.0)),\n",
    "    (\"dof_left_hip_pitch_04\", math.radians(25.0)),\n",
    "    (\"dof_left_hip_roll_03\", math.radians(5.0)),\n",
    "    (\"dof_left_hip_yaw_03\", 0.0),\n",
    "    (\"dof_left_knee_04\", math.radians(50.0)),\n",
    "    (\"dof_left_ankle_02\", math.radians(-25.0)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "3"
   },
   "source": [
    "## Rewards\n",
    "\n",
    "When training a reinforcement learning agent, the most important thing to define is what reward you want the agent to maximimze. `ksim` includes a number of useful default rewards for training walking agents, but it is often a good idea to define new rewards to encourage specific types of behavior. The cell below shows an example of how to define a custom reward. A similar pattern can be used to define custom objectives, events, observations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "@attrs.define(frozen=True, kw_only=True)\n",
    "class JointPositionPenalty(ksim.JointDeviationPenalty):\n",
    "    @classmethod\n",
    "    def create_from_names(\n",
    "        cls,\n",
    "        names: list[str],\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        scale: float = -1.0,\n",
    "        scale_by_curriculum: bool = False,\n",
    "    ) -> Self:\n",
    "        zeros = {k: v for k, v in ZEROS}\n",
    "        joint_targets = [zeros[name] for name in names]\n",
    "\n",
    "        return cls.create(\n",
    "            physics_model=physics_model,\n",
    "            joint_names=tuple(names),\n",
    "            joint_targets=tuple(joint_targets),\n",
    "            scale=scale,\n",
    "            scale_by_curriculum=scale_by_curriculum,\n",
    "        )\n",
    "\n",
    "\n",
    "@attrs.define(frozen=True, kw_only=True)\n",
    "class BentArmPenalty(JointPositionPenalty):\n",
    "    @classmethod\n",
    "    def create_penalty(\n",
    "        cls,\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        scale: float = -1.0,\n",
    "        scale_by_curriculum: bool = False,\n",
    "    ) -> Self:\n",
    "        return cls.create_from_names(\n",
    "            names=[\n",
    "                \"dof_right_shoulder_pitch_03\",\n",
    "                \"dof_right_shoulder_roll_03\",\n",
    "                \"dof_right_shoulder_yaw_02\",\n",
    "                \"dof_right_elbow_02\",\n",
    "                \"dof_right_wrist_00\",\n",
    "                \"dof_left_shoulder_pitch_03\",\n",
    "                \"dof_left_shoulder_roll_03\",\n",
    "                \"dof_left_shoulder_yaw_02\",\n",
    "                \"dof_left_elbow_02\",\n",
    "                \"dof_left_wrist_00\",\n",
    "            ],\n",
    "            physics_model=physics_model,\n",
    "            scale=scale,\n",
    "            scale_by_curriculum=scale_by_curriculum,\n",
    "        )\n",
    "\n",
    "\n",
    "@attrs.define(frozen=True, kw_only=True)\n",
    "class StraightLegPenalty(JointPositionPenalty):\n",
    "    @classmethod\n",
    "    def create_penalty(\n",
    "        cls,\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        scale: float = -1.0,\n",
    "        scale_by_curriculum: bool = False,\n",
    "    ) -> Self:\n",
    "        return cls.create_from_names(\n",
    "            names=[\n",
    "                \"dof_left_hip_pitch_04\",\n",
    "                \"dof_left_hip_roll_03\",\n",
    "                \"dof_left_hip_yaw_03\",\n",
    "                \"dof_right_hip_pitch_04\",\n",
    "                \"dof_right_hip_roll_03\",\n",
    "                \"dof_right_hip_yaw_03\",\n",
    "            ],\n",
    "            physics_model=physics_model,\n",
    "            scale=scale,\n",
    "            scale_by_curriculum=scale_by_curriculum,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "5"
   },
   "source": [
    "## Actor-Critic Model\n",
    "\n",
    "We train our reinforcement learning agent using an RNN-based actor and critic, which we define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "class Actor(eqx.Module):\n",
    "    \"\"\"Actor for the walking task.\"\"\"\n",
    "\n",
    "    input_proj: eqx.nn.Linear\n",
    "    rnns: tuple[eqx.nn.GRUCell, ...]\n",
    "    output_proj: eqx.nn.Linear\n",
    "    num_inputs: int = eqx.static_field()\n",
    "    num_outputs: int = eqx.static_field()\n",
    "    num_mixtures: int = eqx.static_field()\n",
    "    min_std: float = eqx.static_field()\n",
    "    max_std: float = eqx.static_field()\n",
    "    var_scale: float = eqx.static_field()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: PRNGKeyArray,\n",
    "        *,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        min_std: float,\n",
    "        max_std: float,\n",
    "        var_scale: float,\n",
    "        hidden_size: int,\n",
    "        num_mixtures: int,\n",
    "        depth: int,\n",
    "    ) -> None:\n",
    "        # Project input to hidden size\n",
    "        key, input_proj_key = jax.random.split(key)\n",
    "        self.input_proj = eqx.nn.Linear(\n",
    "            in_features=num_inputs,\n",
    "            out_features=hidden_size,\n",
    "            key=input_proj_key,\n",
    "        )\n",
    "\n",
    "        # Create RNN layer\n",
    "        key, rnn_key = jax.random.split(key)\n",
    "        self.rnns = tuple(\n",
    "            [\n",
    "                eqx.nn.GRUCell(\n",
    "                    input_size=hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    key=rnn_key,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Project to output\n",
    "        self.output_proj = eqx.nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_outputs * 3 * num_mixtures,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "        self.var_scale = var_scale\n",
    "\n",
    "    def forward(self, obs_n: Array, carry: Array) -> tuple[distrax.Distribution, Array]:\n",
    "        x_n = self.input_proj(obs_n)\n",
    "        out_carries = []\n",
    "        for i, rnn in enumerate(self.rnns):\n",
    "            x_n = rnn(x_n, carry[i])\n",
    "            out_carries.append(x_n)\n",
    "        out_n = self.output_proj(x_n)\n",
    "\n",
    "        # Reshape the output to be a mixture of gaussians.\n",
    "        slice_len = NUM_JOINTS * self.num_mixtures\n",
    "        mean_nm = out_n[..., :slice_len].reshape(NUM_JOINTS, self.num_mixtures)\n",
    "        std_nm = out_n[..., slice_len : slice_len * 2].reshape(NUM_JOINTS, self.num_mixtures)\n",
    "        logits_nm = out_n[..., slice_len * 2 :].reshape(NUM_JOINTS, self.num_mixtures)\n",
    "\n",
    "        # Softplus and clip to ensure positive standard deviations.\n",
    "        std_nm = jnp.clip((jax.nn.softplus(std_nm) + self.min_std) * self.var_scale, max=self.max_std)\n",
    "\n",
    "        # Apply bias to the means.\n",
    "        mean_nm = mean_nm + jnp.array([v for _, v in ZEROS])[:, None]\n",
    "\n",
    "        dist_n = ksim.MixtureOfGaussians(means_nm=mean_nm, stds_nm=std_nm, logits_nm=logits_nm)\n",
    "\n",
    "        return dist_n, jnp.stack(out_carries, axis=0)\n",
    "\n",
    "\n",
    "class Critic(eqx.Module):\n",
    "    \"\"\"Critic for the walking task.\"\"\"\n",
    "\n",
    "    input_proj: eqx.nn.Linear\n",
    "    rnns: tuple[eqx.nn.GRUCell, ...]\n",
    "    output_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: PRNGKeyArray,\n",
    "        *,\n",
    "        hidden_size: int,\n",
    "        depth: int,\n",
    "    ) -> None:\n",
    "        num_inputs = NUM_CRITIC_INPUTS\n",
    "        num_outputs = 1\n",
    "\n",
    "        # Project input to hidden size\n",
    "        key, input_proj_key = jax.random.split(key)\n",
    "        self.input_proj = eqx.nn.Linear(\n",
    "            in_features=num_inputs,\n",
    "            out_features=hidden_size,\n",
    "            key=input_proj_key,\n",
    "        )\n",
    "\n",
    "        # Create RNN layer\n",
    "        key, rnn_key = jax.random.split(key)\n",
    "        self.rnns = tuple(\n",
    "            [\n",
    "                eqx.nn.GRUCell(\n",
    "                    input_size=hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    key=rnn_key,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Project to output\n",
    "        self.output_proj = eqx.nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_outputs,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_n: Array, carry: Array) -> tuple[Array, Array]:\n",
    "        x_n = self.input_proj(obs_n)\n",
    "        out_carries = []\n",
    "        for i, rnn in enumerate(self.rnns):\n",
    "            x_n = rnn(x_n, carry[i])\n",
    "            out_carries.append(x_n)\n",
    "        out_n = self.output_proj(x_n)\n",
    "\n",
    "        return out_n, jnp.stack(out_carries, axis=0)\n",
    "\n",
    "\n",
    "class Model(eqx.Module):\n",
    "    actor: Actor\n",
    "    critic: Critic\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: PRNGKeyArray,\n",
    "        *,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        min_std: float,\n",
    "        max_std: float,\n",
    "        hidden_size: int,\n",
    "        num_mixtures: int,\n",
    "        depth: int,\n",
    "    ) -> None:\n",
    "        self.actor = Actor(\n",
    "            key,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            min_std=min_std,\n",
    "            max_std=max_std,\n",
    "            var_scale=1.0,\n",
    "            hidden_size=hidden_size,\n",
    "            num_mixtures=num_mixtures,\n",
    "            depth=depth,\n",
    "        )\n",
    "        self.critic = Critic(\n",
    "            key,\n",
    "            hidden_size=hidden_size,\n",
    "            depth=depth,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "7"
   },
   "source": [
    "## Config\n",
    "\n",
    "The [ksim framework](https://github.com/kscalelabs/ksim) is based on [xax](https://github.com/kscalelabs/xax), a JAX training library built by K-Scale. To provide configuration options, xax uses a Config dataclass to parse command-line options. We define the config here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HumanoidWalkingTaskConfig(ksim.PPOConfig):\n",
    "    \"\"\"Config for the humanoid walking task.\"\"\"\n",
    "\n",
    "    # Task parameters.\n",
    "    num_envs: int = xax.field(\n",
    "        value=1,\n",
    "        help=\"The number of environments to run in parallel.\",\n",
    "    )\n",
    "    batch_size: int = xax.field(\n",
    "        value=1,\n",
    "        help=\"The batch size for the PPO training.\",\n",
    "    )\n",
    "\n",
    "    rollout_length_seconds: float = xax.field(\n",
    "        value=1.0,\n",
    "        help=\"The length of the rollout in seconds.\",\n",
    "    )\n",
    "    # Model parameters.\n",
    "    hidden_size: int = xax.field(\n",
    "        value=128,\n",
    "        help=\"The hidden size for the MLPs.\",\n",
    "    )\n",
    "    depth: int = xax.field(\n",
    "        value=5,\n",
    "        help=\"The depth for the MLPs.\",\n",
    "    )\n",
    "    num_mixtures: int = xax.field(\n",
    "        value=5,\n",
    "        help=\"The number of mixtures for the actor.\",\n",
    "    )\n",
    "\n",
    "    # Optimizer parameters.\n",
    "    learning_rate: float = xax.field(\n",
    "        value=3e-4,\n",
    "        help=\"Learning rate for PPO.\",\n",
    "    )\n",
    "    max_grad_norm: float = xax.field(\n",
    "        value=2.0,\n",
    "        help=\"Maximum gradient norm for clipping.\",\n",
    "    )\n",
    "    adam_weight_decay: float = xax.field(\n",
    "        value=1e-5,\n",
    "        help=\"Weight decay for the Adam optimizer.\",\n",
    "    )\n",
    "\n",
    "    # Rendering parameters.\n",
    "    render_track_body_id: int | None = xax.field(\n",
    "        value=0,\n",
    "        help=\"The body id to track with the render camera.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "9"
   },
   "source": [
    "## Task\n",
    "\n",
    "The meat-and-potatoes of our training code is the task. This defines the observations, rewards, model calling logic, and everything else needed by `ksim` to train our reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "class HumanoidWalkingTask(ksim.PPOTask[HumanoidWalkingTaskConfig]):\n",
    "    def get_optimizer(self) -> optax.GradientTransformation:\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(self.config.max_grad_norm),\n",
    "            (\n",
    "                optax.adam(self.config.learning_rate)\n",
    "                if self.config.adam_weight_decay == 0.0\n",
    "                else optax.adamw(self.config.learning_rate, weight_decay=self.config.adam_weight_decay)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def get_mujoco_model(self) -> mujoco.MjModel:\n",
    "        mjcf_path = asyncio.run(ksim.get_mujoco_model_path(\"kbot\", name=\"robot\"))\n",
    "        return mujoco_scenes.mjcf.load_mjmodel(mjcf_path, scene=\"smooth\")\n",
    "\n",
    "    def get_mujoco_model_metadata(self, mj_model: mujoco.MjModel) -> dict[str, JointMetadataOutput]:\n",
    "        metadata = asyncio.run(ksim.get_mujoco_model_metadata(\"kbot\"))\n",
    "        if metadata.joint_name_to_metadata is None:\n",
    "            raise ValueError(\"Joint metadata is not available\")\n",
    "        return metadata.joint_name_to_metadata\n",
    "\n",
    "    def get_actuators(\n",
    "        self,\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        metadata: dict[str, JointMetadataOutput] | None = None,\n",
    "    ) -> ksim.Actuators:\n",
    "        assert metadata is not None, \"Metadata is required\"\n",
    "        return ksim.MITPositionActuators(\n",
    "            physics_model=physics_model,\n",
    "            joint_name_to_metadata=metadata,\n",
    "        )\n",
    "\n",
    "    def get_physics_randomizers(self, physics_model: ksim.PhysicsModel) -> list[ksim.PhysicsRandomizer]:\n",
    "        return [\n",
    "            ksim.StaticFrictionRandomizer(),\n",
    "            ksim.ArmatureRandomizer(),\n",
    "            ksim.AllBodiesMassMultiplicationRandomizer(scale_lower=0.95, scale_upper=1.05),\n",
    "            ksim.JointDampingRandomizer(),\n",
    "            ksim.JointZeroPositionRandomizer(scale_lower=math.radians(-2), scale_upper=math.radians(2)),\n",
    "        ]\n",
    "\n",
    "    def get_events(self, physics_model: ksim.PhysicsModel) -> list[ksim.Event]:\n",
    "        return [\n",
    "            ksim.PushEvent(\n",
    "                x_force=3.0,\n",
    "                y_force=3.0,\n",
    "                z_force=0.3,\n",
    "                force_range=(0.5, 1.0),\n",
    "                x_angular_force=0.1,\n",
    "                y_angular_force=0.1,\n",
    "                z_angular_force=1.0,\n",
    "                interval_range=(0.5, 4.0),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_resets(self, physics_model: ksim.PhysicsModel) -> list[ksim.Reset]:\n",
    "        return [\n",
    "            ksim.RandomJointPositionReset.create(physics_model, {k: v for k, v in ZEROS}, scale=0.1),\n",
    "            ksim.RandomJointVelocityReset(),\n",
    "            ksim.RandomHeadingReset(),\n",
    "        ]\n",
    "\n",
    "    def get_observations(self, physics_model: ksim.PhysicsModel) -> list[ksim.Observation]:\n",
    "        return [\n",
    "            ksim.JointPositionObservation(noise=math.radians(2)),\n",
    "            ksim.JointVelocityObservation(noise=math.radians(10)),\n",
    "            ksim.ActuatorForceObservation(),\n",
    "            ksim.CenterOfMassInertiaObservation(),\n",
    "            ksim.CenterOfMassVelocityObservation(),\n",
    "            ksim.BasePositionObservation(),\n",
    "            ksim.BaseOrientationObservation(),\n",
    "            ksim.BaseLinearVelocityObservation(),\n",
    "            ksim.BaseAngularVelocityObservation(),\n",
    "            ksim.BaseLinearAccelerationObservation(),\n",
    "            ksim.BaseAngularAccelerationObservation(),\n",
    "            ksim.ProjectedGravityObservation.create(\n",
    "                physics_model=physics_model,\n",
    "                framequat_name=\"imu_site_quat\",\n",
    "                lag_range=(0.0, 0.1),\n",
    "                noise=math.radians(1),\n",
    "            ),\n",
    "            ksim.ActuatorAccelerationObservation(),\n",
    "            ksim.BasePositionObservation(),\n",
    "            ksim.BaseOrientationObservation(),\n",
    "            ksim.BaseLinearVelocityObservation(),\n",
    "            ksim.BaseAngularVelocityObservation(),\n",
    "            ksim.CenterOfMassVelocityObservation(),\n",
    "            ksim.SensorObservation.create(\n",
    "                physics_model=physics_model,\n",
    "                sensor_name=\"imu_acc\",\n",
    "                noise=1.0,\n",
    "            ),\n",
    "            ksim.SensorObservation.create(\n",
    "                physics_model=physics_model,\n",
    "                sensor_name=\"imu_gyro\",\n",
    "                noise=math.radians(10),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_commands(self, physics_model: ksim.PhysicsModel) -> list[ksim.Command]:\n",
    "        return []\n",
    "\n",
    "    def get_rewards(self, physics_model: ksim.PhysicsModel) -> list[ksim.Reward]:\n",
    "        return [\n",
    "            # Standard rewards.\n",
    "            ksim.StayAliveReward(scale=1.0),\n",
    "            ksim.UprightReward(scale=1.0),\n",
    "            # Avoid movement penalties.\n",
    "            ksim.AngularVelocityPenalty(index=(\"x\", \"y\", \"z\"), scale=-0.005),\n",
    "            ksim.LinearVelocityPenalty(index=(\"x\", \"y\", \"z\"), scale=-0.005),\n",
    "            # Normalization penalties.\n",
    "            ksim.ActionInBoundsReward.create(physics_model, scale=0.01),\n",
    "            ksim.AvoidLimitsPenalty.create(physics_model, scale=-0.01),\n",
    "            ksim.ActionNearPositionPenalty(joint_threshold=math.radians(2.0), scale=-0.01),\n",
    "            ksim.JointVelocityPenalty(scale=-0.01, scale_by_curriculum=True),\n",
    "            ksim.ActionSmoothnessPenalty(scale=-0.01),\n",
    "            ksim.ActuatorRelativeForcePenalty.create(physics_model, scale=-0.01),\n",
    "            # Bespoke rewards.\n",
    "            BentArmPenalty.create_penalty(physics_model, scale=-0.1),\n",
    "            StraightLegPenalty.create_penalty(physics_model, scale=-0.1),\n",
    "        ]\n",
    "\n",
    "    def get_terminations(self, physics_model: ksim.PhysicsModel) -> list[ksim.Termination]:\n",
    "        return [\n",
    "            ksim.BadZTermination(unhealthy_z_lower=0.4, unhealthy_z_upper=1.6),\n",
    "            ksim.NotUprightTermination(max_radians=math.radians(60)),\n",
    "            ksim.HighVelocityTermination(),\n",
    "            ksim.FarFromOriginTermination(max_dist=10.0),\n",
    "        ]\n",
    "\n",
    "    def get_curriculum(self, physics_model: ksim.PhysicsModel) -> ksim.Curriculum:\n",
    "        return ksim.LinearCurriculum(\n",
    "            step_size=0.01,\n",
    "            step_every_n_epochs=10,\n",
    "        )\n",
    "\n",
    "    def get_model(self, key: PRNGKeyArray) -> Model:\n",
    "        return Model(\n",
    "            key,\n",
    "            num_inputs=NUM_ACTOR_INPUTS,\n",
    "            num_outputs=NUM_JOINTS,\n",
    "            min_std=0.01,\n",
    "            max_std=1.0,\n",
    "            hidden_size=self.config.hidden_size,\n",
    "            num_mixtures=self.config.num_mixtures,\n",
    "            depth=self.config.depth,\n",
    "        )\n",
    "\n",
    "    def run_actor(\n",
    "        self,\n",
    "        model: Actor,\n",
    "        observations: xax.FrozenDict[str, Array],\n",
    "        commands: xax.FrozenDict[str, Array],\n",
    "        carry: Array,\n",
    "    ) -> tuple[distrax.Distribution, Array]:\n",
    "        joint_pos_n = observations[\"joint_position_observation\"]\n",
    "        joint_vel_n = observations[\"joint_velocity_observation\"]\n",
    "        proj_grav_3 = observations[\"projected_gravity_observation\"]\n",
    "        imu_acc_3 = observations[\"sensor_observation_imu_acc\"]\n",
    "        imu_gyro_3 = observations[\"sensor_observation_imu_gyro\"]\n",
    "\n",
    "        obs_n = jnp.concatenate(\n",
    "            [\n",
    "                joint_pos_n,  # NUM_JOINTS\n",
    "                joint_vel_n,  # NUM_JOINTS\n",
    "                proj_grav_3,  # 3\n",
    "                imu_acc_3,  # 3\n",
    "                imu_gyro_3,  # 3\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        action, carry = model.forward(obs_n, carry)\n",
    "\n",
    "        return action, carry\n",
    "\n",
    "    def run_critic(\n",
    "        self,\n",
    "        model: Critic,\n",
    "        observations: xax.FrozenDict[str, Array],\n",
    "        commands: xax.FrozenDict[str, Array],\n",
    "        carry: Array,\n",
    "    ) -> tuple[Array, Array]:\n",
    "        dh_joint_pos_j = observations[\"joint_position_observation\"]\n",
    "        dh_joint_vel_j = observations[\"joint_velocity_observation\"]\n",
    "        com_inertia_n = observations[\"center_of_mass_inertia_observation\"]\n",
    "        com_vel_n = observations[\"center_of_mass_velocity_observation\"]\n",
    "        imu_acc_3 = observations[\"sensor_observation_imu_acc\"]\n",
    "        imu_gyro_3 = observations[\"sensor_observation_imu_gyro\"]\n",
    "        proj_grav_3 = observations[\"projected_gravity_observation\"]\n",
    "        act_frc_obs_n = observations[\"actuator_force_observation\"]\n",
    "        base_pos_3 = observations[\"base_position_observation\"]\n",
    "        base_quat_4 = observations[\"base_orientation_observation\"]\n",
    "\n",
    "        obs_n = jnp.concatenate(\n",
    "            [\n",
    "                dh_joint_pos_j,  # NUM_JOINTS\n",
    "                dh_joint_vel_j / 10.0,  # NUM_JOINTS\n",
    "                com_inertia_n,  # 160\n",
    "                com_vel_n,  # 96\n",
    "                imu_acc_3,  # 3\n",
    "                imu_gyro_3,  # 3\n",
    "                proj_grav_3,  # 3\n",
    "                act_frc_obs_n / 100.0,  # NUM_JOINTS\n",
    "                base_pos_3,  # 3\n",
    "                base_quat_4,  # 4\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        return model.forward(obs_n, carry)\n",
    "\n",
    "    def get_ppo_variables(\n",
    "        self,\n",
    "        model: Model,\n",
    "        trajectory: ksim.Trajectory,\n",
    "        model_carry: tuple[Array, Array],\n",
    "        rng: PRNGKeyArray,\n",
    "    ) -> tuple[ksim.PPOVariables, tuple[Array, Array]]:\n",
    "        def scan_fn(\n",
    "            actor_critic_carry: tuple[Array, Array],\n",
    "            transition: ksim.Trajectory,\n",
    "        ) -> tuple[tuple[Array, Array], ksim.PPOVariables]:\n",
    "            actor_carry, critic_carry = actor_critic_carry\n",
    "            actor_dist, next_actor_carry = self.run_actor(\n",
    "                model=model.actor,\n",
    "                observations=transition.obs,\n",
    "                commands=transition.command,\n",
    "                carry=actor_carry,\n",
    "            )\n",
    "            log_probs = actor_dist.log_prob(transition.action)\n",
    "            assert isinstance(log_probs, Array)\n",
    "            value, next_critic_carry = self.run_critic(\n",
    "                model=model.critic,\n",
    "                observations=transition.obs,\n",
    "                commands=transition.command,\n",
    "                carry=critic_carry,\n",
    "            )\n",
    "\n",
    "            transition_ppo_variables = ksim.PPOVariables(\n",
    "                log_probs=log_probs,\n",
    "                values=value.squeeze(-1),\n",
    "            )\n",
    "\n",
    "            next_carry = jax.tree.map(\n",
    "                lambda x, y: jnp.where(transition.done, x, y),\n",
    "                self.get_initial_model_carry(rng),\n",
    "                (next_actor_carry, next_critic_carry),\n",
    "            )\n",
    "\n",
    "            return next_carry, transition_ppo_variables\n",
    "\n",
    "        next_model_carry, ppo_variables = jax.lax.scan(scan_fn, model_carry, trajectory)\n",
    "\n",
    "        return ppo_variables, next_model_carry\n",
    "\n",
    "    def get_initial_model_carry(self, rng: PRNGKeyArray) -> tuple[Array, Array]:\n",
    "        return (\n",
    "            jnp.zeros(shape=(self.config.depth, self.config.hidden_size)),\n",
    "            jnp.zeros(shape=(self.config.depth, self.config.hidden_size)),\n",
    "        )\n",
    "\n",
    "    def sample_action(\n",
    "        self,\n",
    "        model: Model,\n",
    "        model_carry: tuple[Array, Array],\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        physics_state: ksim.PhysicsState,\n",
    "        observations: xax.FrozenDict[str, Array],\n",
    "        commands: xax.FrozenDict[str, Array],\n",
    "        rng: PRNGKeyArray,\n",
    "        argmax: bool,\n",
    "    ) -> ksim.Action:\n",
    "        actor_carry_in, critic_carry_in = model_carry\n",
    "\n",
    "        # Runs the actor model to get the action distribution.\n",
    "        action_dist_j, actor_carry = self.run_actor(\n",
    "            model=model.actor,\n",
    "            observations=observations,\n",
    "            commands=commands,\n",
    "            carry=actor_carry_in,\n",
    "        )\n",
    "        action_j = action_dist_j.mode() if argmax else action_dist_j.sample(seed=rng)\n",
    "\n",
    "        return ksim.Action(\n",
    "            action=action_j,\n",
    "            carry=(actor_carry, critic_carry_in),\n",
    "            aux_outputs=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf85a7",
   "metadata": {},
   "source": [
    "# Launch TensorBoard\n",
    "\n",
    "The below cell launches TensorBoard to visualize the training progress.\n",
    "\n",
    "After launching an experiment, please for wait 5 minutes for the task to start running and then click the reload button on the top right corner of the TensorBoard page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir humanoid_walking_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "11"
   },
   "source": [
    "## Launching an Experiment\n",
    "\n",
    "To launch an experiment with `xax`, you can use `Task.launch(config)`. Note that this is usually intended to be called from the command-line, so it will by default attempt to parse additional command-line arguments unless `use_cli=False` is set.\n",
    "\n",
    "By default, runs will be logged to a directory called `run_[x]` in the task directory (/content/humanoid_walking_task/ in Colab). From there, you can download the ckpt `.bin` files and the TensorBoard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12",
    "outputId": "b37b0dca-fc55-445b-f175-4f4d533bd22b"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    HumanoidWalkingTask.launch(\n",
    "        HumanoidWalkingTaskConfig(\n",
    "            # Training parameters.\n",
    "            num_envs=2048,\n",
    "            batch_size=256,\n",
    "            num_passes=4,\n",
    "            epochs_per_log_step=1,\n",
    "            rollout_length_seconds=8.0,\n",
    "            # Simulation parameters.\n",
    "            dt=0.002,\n",
    "            ctrl_dt=0.02,\n",
    "            iterations=8,\n",
    "            ls_iterations=8,\n",
    "            max_action_latency=0.01,\n",
    "            # Checkpointing parameters.\n",
    "            save_every_n_seconds=60,\n",
    "        ),\n",
    "        use_cli=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
